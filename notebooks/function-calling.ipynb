{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling 101\n",
    "One of the struggles of using LLMs like ChatGPT is that they do not produce a structured data output. This is important for programmatic systems that largely rely on structured data for system interaction. For example, if you want to build a program that analyzes the sentiment of a movie review, you might have to execute a prompt that looks like the following:\n",
    "\n",
    "```\n",
    "prompt = f'''\n",
    "Please perform a sentiment analysis on the following movie review:\n",
    "{MOVIE_REVIEW_TEXT}\n",
    "Please output your response as a single word: either \"Positive\" or \"Negative\". Do not add any extra characters.\n",
    "'''\n",
    "```\n",
    "\n",
    "The problem with this is that it doesn't always work. It's pretty common that the LLM will throw in an undesired period or longer explanation like: \"The sentiment of this movie is: Positive.\" While you can regex out the answer (ðŸ¤¢), this is obviously not ideal. What would be ideal is if the LLM would return the output as something like the following structured JSON:\n",
    "\n",
    "```\n",
    "{\n",
    "    'sentiment': 'positive'\n",
    "}\n",
    "```\n",
    "\n",
    "Enter **OpenAI's new function calling**! Function calling is precisely the answer to the problem above. This Jupyter notebook will demonstrate a simple example of how to use OpenAI's new function calling in Python. If you would like to see the full documentation, [please check out this link](https://platform.openai.com/docs/guides/gpt/function-calling)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Let's start with our imports. Now, you may already have the `openai` Python client already installed, but you'll most likely need to upgrade it to get the new function calling functionality. Here's how to do this upgrade in your Terminal / Powershell with `pip`:\n",
    "\n",
    "```\n",
    "pip install openai --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the API key and organization ID from file (NOT pushed to GitHub)\n",
    "with open('../keys/openai-keys.yaml') as f:\n",
    "    keys_yaml = yaml.safe_load(f)\n",
    "\n",
    "# Applying our API key and organization ID to OpenAI\n",
    "openai.organization = keys_yaml['ORG_ID']\n",
    "openai.api_key = keys_yaml['API_KEY']\n",
    "os.environ['OPENAI_API_KEY'] = keys_yaml['API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test out the function calling functionality, I wrote a short \"About Me\" containing particular facts that we'll be parsing out into appropriate data structures, including integers and strings. Let's load in this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is David Hundley. I am a principal machine learning engineer at State Farm. I enjoy learning about AI and teaching what I learn back to others. I have two daughters. I drive a Tesla Model 3, and my favorite video game series is The Legend of Zelda.\n"
     ]
    }
   ],
   "source": [
    "# Loading the \"About Me\" text from local file\n",
    "with open('../data/about-me.txt', 'r') as f:\n",
    "    about_me = f.read()\n",
    "\n",
    "print(about_me)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pre-Function Calling Days\n",
    "Before we demonstrate function calling, let's demonstrate how we used to use prompt engineering and Regex to produce a structure JSON that we can programmatically work with down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering a prompt to extract as much information from \"About Me\" as a JSON object\n",
    "about_me_prompt = f'''\n",
    "Please extract information as a JSON object. Please look for the following pieces of information.\n",
    "Name\n",
    "Job title\n",
    "Company\n",
    "Number of children as a single number\n",
    "Car make\n",
    "Car model\n",
    "Favorite video game series\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{about_me}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
    "openai_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [{'role': 'user', 'content': about_me_prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'David Hundley',\n",
       " 'Job title': 'Principal Machine Learning Engineer',\n",
       " 'Company': 'State Farm',\n",
       " 'Number of children': 2,\n",
       " 'Car make': 'Tesla',\n",
       " 'Car model': 'Model 3',\n",
       " 'Favorite video game series': 'The Legend of Zelda'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response = json.loads(openai_response['choices'][0]['message']['content'])\n",
    "json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the New Function Calling Capabilities\n",
    "Now that we've demonstrated how we used to get structured JSON in the pre-function calling days, let's move into how we can now make use of function calling to extract the same results but in a more consistent manner for our systematic usage. We'll start more simply with a single custom function and then address a few more \"advanced\" functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our initial extract_person_info function\n",
    "def extract_person_info(name, job_title, num_children):\n",
    "    '''\n",
    "    Prints basic \"About Me\" information\n",
    "\n",
    "    Inputs:\n",
    "        name (str): Name of the person\n",
    "        job_title (str): Job title of the person\n",
    "        num_chilren (int): The number of children the parent has.\n",
    "    '''\n",
    "    \n",
    "    print(f'This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining how we want ChatGPT to call our custom functions\n",
    "my_custom_functions = [\n",
    "    {\n",
    "        'name': 'extract_person_info',\n",
    "        'description': 'Get \"About Me\" information from the body of the input text',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the person'\n",
    "                },\n",
    "                'job_title': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Job title of the person'\n",
    "                },\n",
    "                'num_children': {\n",
    "                    'type': 'integer',\n",
    "                    'description': 'Number of children the person is a parent to'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7aSokNbBAHjNFOMAnGqA7uuXsmLjr\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1688924938,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"extract_person_info\",\n",
      "          \"arguments\": \"{\\n  \\\"name\\\": \\\"David Hundley\\\",\\n  \\\"job_title\\\": \\\"Principal Machine Learning Engineer\\\",\\n  \\\"num_children\\\": 2\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 147,\n",
      "    \"completion_tokens\": 37,\n",
      "    \"total_tokens\": 184\n",
      "  }\n",
      "}\n",
      "{'name': 'David Hundley', 'job_title': 'Principal Machine Learning Engineer', 'num_children': 2}\n"
     ]
    }
   ],
   "source": [
    "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
    "openai_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [{'role': 'user', 'content': about_me}],\n",
    "    functions = my_custom_functions,\n",
    "    function_call = 'auto'\n",
    ")\n",
    "\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the prompt I submit doesn't contain the information I want to extract per my custom function?\n",
    "In our original example, our custom function sought to extract three very specific bits of information, and we demonstrated that this worked successfully by passing in my custom \"About Me\" text as a prompt. But you might be wondering, what happens if you pass in any other prompt that doesn't contain that information?\n",
    "\n",
    "Recall that we set a parameter in our API client call called function_call that we set to auto. We'll explore this even deeper in the next subsection, but what this parameter is essentially doing is telling ChatGPT to use its best judgment in figuring out when to structure the output for one of our custom functions.\n",
    "\n",
    "So what happens when we submit a prompt that doesn't match any of our custom functions? Simply put, it defaults to typical behavior as if function calling doesn't exist. Let's test this out with an arbitrary prompt: \"How tall is the Eiffel Tower?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7aSywAKIMPCUT2mCNxoJM0OkwYqLJ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1688925570,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"The Eiffel Tower is approximately 330 meters (1,083 feet) tall.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 97,\n",
      "    \"completion_tokens\": 19,\n",
      "    \"total_tokens\": 116\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
    "openai_response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [{'role': 'user', 'content': 'How tall is the Eiffel Tower?'}],\n",
    "    functions = my_custom_functions,\n",
    "    function_call = 'auto'\n",
    ")\n",
    "\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to extract only vehicle information\n",
    "def extract_vehicle_info(vehicle_make, vehicle_model):\n",
    "    '''\n",
    "    Prints basic vehicle information\n",
    "\n",
    "    Inputs:\n",
    "        - vehicle_make (str): Make of the vehicle\n",
    "        - vehicle_model (str): Model of the vehicle\n",
    "    '''\n",
    "    \n",
    "    print(f'Vehicle make: {vehicle_make}\\nVehicle model: {vehicle_model}')\n",
    "\n",
    "\n",
    "\n",
    "# Defining a function to extract all information provided in the original \"About Me\" prompt\n",
    "def extract_all_info(name, job_title, num_children, vehicle_make, vehicle_model, company_name, favorite_vg_series):\n",
    "    '''\n",
    "    Prints the full \"About Me\" information\n",
    "\n",
    "    Inputs:\n",
    "        - name (str): Name of the person\n",
    "        - job_title (str): Job title of the person\n",
    "        - num_chilren (int): The number of children the parent has\n",
    "        - vehicle_make (str): Make of the vehicle\n",
    "        - vehicle_model (str): Model of the vehicle\n",
    "        - company_name (str): Name of the company the person works for\n",
    "        - favorite_vg_series (str): Person's favorite video game series.\n",
    "    '''\n",
    "    \n",
    "    print(f'''\n",
    "    This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.\n",
    "    They drive a {vehicle_make} {vehicle_model}.\n",
    "    They work for {company_name}.\n",
    "    Their favorite video game series is {favorite_vg_series}.\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining how we want ChatGPT to call our custom functions\n",
    "my_custom_functions = [\n",
    "    {\n",
    "        'name': 'extract_person_info',\n",
    "        'description': 'Get \"About Me\" information from the body of the input text',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the person'\n",
    "                },\n",
    "                'job_title': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Job title of the person'\n",
    "                },\n",
    "                'num_children': {\n",
    "                    'type': 'integer',\n",
    "                    'description': 'Number of children the person is a parent to'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'extract_vehicle_info',\n",
    "        'description': 'Extract the make and model of the person\\'s car',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'vehicle_make': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Make of the person\\'s vehicle'\n",
    "                },\n",
    "                'vehicle_model': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Model of the person\\'s vehicle'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'extract_all_info',\n",
    "        'description': 'Extract all information about a person including their vehicle make and model',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the person'\n",
    "                },\n",
    "                'job_title': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Job title of the person'\n",
    "                },\n",
    "                'num_children': {\n",
    "                    'type': 'integer',\n",
    "                    'description': 'Number of children the person is a parent to'\n",
    "                },\n",
    "                'vehicle_make': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Make of the person\\'s vehicle'\n",
    "                },\n",
    "                'vehicle_model': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Model of the person\\'s vehicle'\n",
    "                },\n",
    "                'company_name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the company the person works for'\n",
    "                },\n",
    "                'favorite_vg_series': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the person\\'s favorite video game series'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's demonstrate what happens when we apply 3 different samples to all of the custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a list of samples\n",
    "samples = [\n",
    "    str(about_me),\n",
    "    'My name is David Hundley. I am a principal machine learning engineer, and I have two daughters.',\n",
    "    'She drives a Kia Sportage.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #1's results:\n",
      "{\n",
      "  \"id\": \"chatcmpl-7aTjJLoSCkICSQZM2Eab3HG3IbGca\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1688928445,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"extract_all_info\",\n",
      "          \"arguments\": \"{\\n  \\\"name\\\": \\\"David Hundley\\\",\\n  \\\"job_title\\\": \\\"principal machine learning engineer\\\",\\n  \\\"num_children\\\": 2,\\n  \\\"vehicle_make\\\": \\\"Tesla\\\",\\n  \\\"vehicle_model\\\": \\\"Model 3\\\",\\n  \\\"company_name\\\": \\\"State Farm\\\",\\n  \\\"favorite_vg_series\\\": \\\"The Legend of Zelda\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 320,\n",
      "    \"completion_tokens\": 77,\n",
      "    \"total_tokens\": 397\n",
      "  }\n",
      "}\n",
      "Sample #2's results:\n",
      "{\n",
      "  \"id\": \"chatcmpl-7aTjLnQMyvRBpfpukfM2SYXKMuI5C\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1688928447,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"extract_person_info\",\n",
      "          \"arguments\": \"{\\n  \\\"name\\\": \\\"David Hundley\\\",\\n  \\\"job_title\\\": \\\"principal machine learning engineer\\\",\\n  \\\"num_children\\\": 2\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 282,\n",
      "    \"completion_tokens\": 37,\n",
      "    \"total_tokens\": 319\n",
      "  }\n",
      "}\n",
      "Sample #3's results:\n",
      "{\n",
      "  \"id\": \"chatcmpl-7aTjMPXvNHG05xxsgSPkgoDreHNkO\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1688928448,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"extract_vehicle_info\",\n",
      "          \"arguments\": \"{\\n  \\\"vehicle_make\\\": \\\"Kia\\\",\\n  \\\"vehicle_model\\\": \\\"Sportage\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 268,\n",
      "    \"completion_tokens\": 27,\n",
      "    \"total_tokens\": 295\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Iterating over the three samples\n",
    "for i, sample in enumerate(samples):\n",
    "    \n",
    "    print(f'Sample #{i + 1}\\'s results:')\n",
    "\n",
    "    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
    "    openai_response = openai.ChatCompletion.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = [{'role': 'user', 'content': sample}],\n",
    "        functions = my_custom_functions,\n",
    "        function_call = 'auto'\n",
    "    )\n",
    "\n",
    "    # Printing the sample's response\n",
    "    print(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each of the respective prompts, ChatGPT selected the correct custom function, and we can specifically note that in the `name` value under `function_call` in the API's response object. In addition to this being a handy way to identify which function to use the arguments for, we can programmatically map our actual custom Python function to this value to run the correct code appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #1's results:\n",
      "\n",
      "    This person's name is David Hundley. Their job title is principal machine learning engineer, and they have 2 children.\n",
      "    They drive a Tesla Model 3.\n",
      "    They work for State Farm.\n",
      "    Their favorite video game series is The Legend of Zelda.\n",
      "    \n",
      "Sample #2's results:\n",
      "This person's name is David Hundley. Their job title is Principal Machine Learning Engineer, and they have 2 children.\n",
      "Sample #3's results:\n",
      "Vehicle make: Kia\n",
      "Vehicle model: Sportage\n"
     ]
    }
   ],
   "source": [
    "# Iterating over the three samples\n",
    "for i, sample in enumerate(samples):\n",
    "    \n",
    "    print(f'Sample #{i + 1}\\'s results:')\n",
    "\n",
    "    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
    "    openai_response = openai.ChatCompletion.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = [{'role': 'user', 'content': sample}],\n",
    "        functions = my_custom_functions,\n",
    "        function_call = 'auto'\n",
    "    )['choices'][0]['message']\n",
    "\n",
    "    # Checking to see that a function call was invoked\n",
    "    if openai_response.get('function_call'):\n",
    "\n",
    "        # Checking to see which specific function call was invoked\n",
    "        function_called = openai_response['function_call']['name']\n",
    "\n",
    "        # Extracting the arguments of the function call\n",
    "        function_args = json.loads(openai_response['function_call']['arguments'])\n",
    "\n",
    "        # Invoking the proper functions\n",
    "        if function_called == 'extract_person_info':\n",
    "            extract_person_info(*list(function_args.values()))\n",
    "        elif function_called == 'extract_vehicle_info':\n",
    "            extract_vehicle_info(*list(function_args.values()))\n",
    "        elif function_called == 'extract_all_info':\n",
    "            extract_all_info(*list(function_args.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Function Calling with LangChain\n",
    "Given its popularity amongst the Generative AI community, I thought I'd re-visit this notebook and add some code to show how you might make use of this exact same functionality in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the LangChain objects\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the proper instance of the OpenAI model\n",
    "llm = ChatOpenAI(model = 'gpt-3.5-turbo-0613')\n",
    "\n",
    "# Setting a LangChain ChatPromptTemplate\n",
    "chat_prompt_template = ChatPromptTemplate.from_template('{my_prompt}')\n",
    "\n",
    "# Setting the JSON schema for extracting vehicle information\n",
    "langchain_json_schema = {\n",
    "    'name': 'extract_vehicle_info',\n",
    "    'description': 'Extract the make and model of the person\\'s car',\n",
    "    'type': 'object',\n",
    "    'properties': {\n",
    "        'vehicle_make': {\n",
    "            'title': 'Vehicle Make',\n",
    "            'type': 'string',\n",
    "            'description': 'Make of the person\\'s vehicle'\n",
    "        },\n",
    "        'vehicle_model': {\n",
    "            'title': 'Vehicle Model',\n",
    "            'type': 'string',\n",
    "            'description': 'Model of the person\\'s vehicle'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LangChain chain object for function calling\n",
    "chain = create_structured_output_chain(output_schema = langchain_json_schema,\n",
    "                                       llm = llm,\n",
    "                                       prompt = chat_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vehicle_make': 'Tesla', 'vehicle_model': 'Model 3'}\n"
     ]
    }
   ],
   "source": [
    "# Getting results with a demo prompt\n",
    "print(chain.run(my_prompt = 'I drive a Tesla Model 3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
